{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-ZJefpAeH1qX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import h5py,pandas as pd, csv\n",
    "import matplotlib.pyplot as plt, numpy as np\n",
    "import os.path,sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale, StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquisition des données:  \n",
    "*X_train* est le dataset d'entrainement, de taille (946, 40, 7, 500) (classe correspondantes dans *y_train*, de taille (946,))  \n",
    "*X_test* est le dataset à classifier, de taille (946, 40, 7, 500)"
   ]
  },
  {
   "source": [
    "X_file=h5py.File(\"data/X_train_new.h5\",'r') # X est ici au format \"HDF5 dataset\"\n",
    "X_train=np.array(X_file['features']) # X est ici au format \"ndarray\"\n",
    "\n",
    "X_final_file=h5py.File(\"data/X_test_new.h5\",'r') # X est ici au format \"HDF5 dataset\"\n",
    "X_test=np.array(X_final_file['features']) # X est ici au \"ndarray\"\n",
    "\n",
    "y=pd.read_csv(\"data/y_train_AvCsavx.csv\") # y est ici au format \"pandas DataFrame\"\n",
    "y_train=np.array(y['label']) # y est ici au format \"ndarray\""
   ],
   "cell_type": "code",
   "metadata": {
    "scrolled": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "Préprocessing des données"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "transform = StandardScaler().fit_transform #fonction de standardisation\n",
    "for j in range(40):\n",
    "    for i in range(X_train.shape[0]):\n",
    "        X_train[i,j,:,:] = np.transpose(transform(np.transpose(X_train[i,j,:,:])))\n",
    "    for i in range(X_test.shape[0]):\n",
    "        X_test[i,j,:,:] = np.transpose(transform(np.transpose(X_test[i,j,:,:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train=np.concatenate(X_train, axis=0)  #sépération des 40 fenêtres indépendantes (comme si chaque fenêtre correspondait à une personne)\n",
    "y_train=np.repeat(y_train, 40)  #Multiplication par 40 de chaque personne (car séparation des fenêtres)\n",
    "X_train, y_train = shuffle(X_train, y_train) #mélange des données pour ne pas avoir 40 fois le même sexe d'affilé"
   ]
  },
  {
   "source": [
    "Rééquilibrage du dataset de training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_h = np.where(y_train==0)[0] #indices de tous les hommes\n",
    "mask_f = np.where(y_train==1)[0] #indices de toutes les femmes\n",
    "nb_h = len(mask_h) #nombre d'hommes\n",
    "nb_f = len(mask_f) #nombre de femmes"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = (nb_h - nb_f)//2\n",
    "mask_h = mask_h[:nb] #liste de taille (nb_h - nb_f)/2 d'indices d'hommes ) à supprimer\n",
    "mask_f = np.resize(mask_f, nb) #liste de taille (nb_h - nb_f)/2 d'indices de femmes à rajouter (potentiellement répétés)"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_X = X_train[mask_f,...] #liste des femmes à rajouter\n",
    "add_y = y_train[mask_f]\n",
    "X_train = np.delete(X_train, mask_h, 0) #suppression de la liste des hommes\n",
    "y_train = np.delete(y_train, mask_h, 0)\n",
    "X_train=np.concatenate((X_train, add_X), axis=0) #ajout de la liste de femmes\n",
    "y_train=np.concatenate((y_train, add_y), axis=0)"
   ]
  },
  {
   "source": [
    "Création du CNN"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "[\n",
    "    layers.Conv2D(filters=25, kernel_size=(1,10), strides=1, padding='same', input_shape=(7,500,1)),\n",
    "    layers.Conv2D(filters=25, kernel_size=(7,1), strides=1, padding='same', use_bias=False),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=0.00001),\n",
    "    layers.Activation('elu'),\n",
    "    layers.MaxPool2D(pool_size=(1,3), strides=(1,3), padding='same'),\n",
    "    \n",
    "    layers.Dropout(0.5),\n",
    "    layers.Conv2D(filters=50, kernel_size=(1,10), strides=1, padding='same', use_bias=False),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=0.00001),\n",
    "    layers.Activation('elu'),\n",
    "    layers.MaxPool2D(pool_size=(1,3), strides=(1,3), padding='same'),\n",
    "\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Conv2D(filters=100, kernel_size=(1,10), strides=1, padding='same', use_bias=False),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=0.00001),\n",
    "    layers.Activation('elu'),\n",
    "    layers.MaxPool2D(pool_size=(1,3), strides=(1,3), padding='same'),\n",
    "\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Conv2D(filters=200, kernel_size=(1,10), strides=1, padding='same', use_bias=False),\n",
    "    layers.BatchNormalization(momentum=0.1, epsilon=0.00001),\n",
    "    layers.Activation('elu'),\n",
    "    layers.MaxPool2D(pool_size=(1,3), strides=(1,3), padding='same'),\n",
    "\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "]\n",
    ")"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "loss='binary_crossentropy',\n",
    "optimizer=keras.optimizers.Adamax(learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-07),\n",
    "metrics=[keras.metrics.BinaryAccuracy(name='accuracy'), keras.metrics.AUC(name='AUC')]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "Entrainement du modèle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=2\n",
    "batch_size=69\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Application du modèle aux données à classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[]\n",
    "for i in range(40):\n",
    "    X_test_i=X_test[:,i,:,:]\n",
    "    X_test_i=X_test_i.reshape(X_test_i.shape[0], X_test_i.shape[1], X_test_i.shape[2], 1)\n",
    "    y_pred.append(model.predict(X_test_i))\n",
    "y_pred=np.mean(y_pred, axis=0)"
   ]
  },
  {
   "source": [
    "Export de la classification au format CSV"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('classification.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"id\", \"label\"])\n",
    "    for i in range(len(y_pred)):\n",
    "        writer.writerow([i, int(y_pred[i]>0.5)])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}