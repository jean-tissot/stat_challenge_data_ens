{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-ZJefpAeH1qX",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import h5py,pandas as pd, numpy as np\n",
    "import os.path\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale, StandardScaler, MinMaxScaler\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aquisition des données pour utilisation dans Jupyter (local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from data import dataread, datatreat_A1\n",
    "from test import test_1\n",
    "from tools import save_model, save_results, plot_loss_acc_history, loss_generator, save_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y, X_final = dataread()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN deep ConvNet\n",
    "def cnn_4(loss='binary_crossentropy'):\n",
    "  model = keras.Sequential(\n",
    "    [\n",
    "      layers.Conv2D(filters=25, kernel_size=(1,10), strides=1, padding='same', input_shape=(7,500,1)),\n",
    "      layers.Conv2D(filters=25, kernel_size=(7,1), strides=1, padding='same', use_bias=False),\n",
    "      layers.BatchNormalization(momentum=0.1, epsilon=0.00001),\n",
    "      layers.Activation('elu'),\n",
    "      layers.MaxPool2D(pool_size=(1,3), strides=(1,3), padding='same'),\n",
    "      \n",
    "      layers.Dropout(0.5),\n",
    "      layers.Conv2D(filters=50, kernel_size=(1,10), strides=1, padding='same', use_bias=False),\n",
    "      layers.BatchNormalization(momentum=0.1, epsilon=0.00001),\n",
    "      layers.Activation('elu'),\n",
    "      layers.MaxPool2D(pool_size=(1,3), strides=(1,3), padding='same'),\n",
    "\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Conv2D(filters=100, kernel_size=(1,10), strides=1, padding='same', use_bias=False),\n",
    "      layers.BatchNormalization(momentum=0.1, epsilon=0.00001),\n",
    "      layers.Activation('elu'),\n",
    "      layers.MaxPool2D(pool_size=(1,3), strides=(1,3), padding='same'),\n",
    "\n",
    "      layers.Dropout(0.5),\n",
    "      layers.Conv2D(filters=200, kernel_size=(1,10), strides=1, padding='same', use_bias=False),\n",
    "      layers.BatchNormalization(momentum=0.1, epsilon=0.00001),\n",
    "      layers.Activation('elu'),\n",
    "      layers.MaxPool2D(pool_size=(1,3), strides=(1,3), padding='same'),\n",
    "\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(1, activation='sigmoid')\n",
    "    ]\n",
    "  )\n",
    "  model.compile(\n",
    "    loss=loss,\n",
    "    optimizer=keras.optimizers.Adamax(learning_rate=0.002, beta_1=0.9, beta_2=0.999, epsilon=1e-07),\n",
    "    metrics=[keras.metrics.BinaryAccuracy(name='accuracy'), keras.metrics.AUC(name='AUC')]\n",
    "  )\n",
    "  model.summary()\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs=200\n",
    "batch_size=70\n",
    "validation_split=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La proportion H/F des données d'entraînement est de 1.0\n",
      "L'échantillon de training comporte 30240 frames\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7fcf8fca14aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprop_HF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatatreat_A1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mShuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Standardization\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"50/50\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbalancing_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"duplicate/remove\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 4)"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test, prop_HF = datatreat_A1(x0=X, y0=y, x_test=None, train_size=0.8, Shuffle=True, preprocess=\"Standardization\", ratio=\"50/50\", balancing_method=\"duplicate/remove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id='cnn_4_standardized_duprem'\n",
    "model = cnn_4()\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "accuracy, roc, f1_macro, f1_wei = test_1(model, X_test, y_test, id)\n",
    "save_results(id, 'datatreat_A1 Standardization duplicate/remove', 'cnn_1', epochs, batch_size, accuracy, roc, f1_macro, f1_wei, validation_split)\n",
    "plot_loss_acc_history(history, id, validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs=150\n",
    "batch_size=70\n",
    "validation_split=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La proportion H/F des données d'entraînement est de 1.0\n",
      "L'échantillon de training comporte 37840 frames\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, prop_HF = datatreat_A1(x0=X, y0=y, x_test=X_final, train_size=0.8, Shuffle=True, preprocess=\"Standardization\", ratio=\"50/50\", balancing_method=\"duplicate/remove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id='cnn_4_standardized_duprem'\n",
    "model = cnn_4()\n",
    "history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size)\n",
    "save_model(model, id)\n",
    "y_pred = test_1(model=model, X_test=X_test, id=id)\n",
    "save_submission(y_pred, id)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
